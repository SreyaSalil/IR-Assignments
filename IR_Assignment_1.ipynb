{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN19mp6tDwI9jRlOj+Bfk1/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SreyaSalil/IR-Assignments/blob/main/IR_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW0w9SRcdEw2"
      },
      "source": [
        "# **IR Assignment 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13PEI35jdJad"
      },
      "source": [
        "*Pre-processing of a Text Document: Accent removal, stop word removal and stemming. Step by step pre-processing including necessary statistics at each step.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjeiAHvQdfT0"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HoqYB2dfin1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25bd610c-5106-4734-8ea5-46af27777add"
      },
      "source": [
        "import string\n",
        "#For acessing files\n",
        "import os,sys\n",
        "# To strip HTML tags\n",
        "from bs4 import BeautifulSoup\n",
        "#To remove Numbers in text using RE\n",
        "import re\n",
        "#accent removal\n",
        "import unicodedata\n",
        "#stop word removal and word tokenization\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "#Stemming\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erCC0-_eGFg6"
      },
      "source": [
        "## Document Preprocessing Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cOmEvsW5N7V"
      },
      "source": [
        "### Remove closed and unclosed HTML tags in document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Gu_Y9fmlNN"
      },
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlbERm7s5Zbd"
      },
      "source": [
        "### Accent Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7cFE0dLxA8t"
      },
      "source": [
        "# Remove non-ASCII and accented characters (é, â, î, ñ or ô) from list of tokenized words\n",
        "def remove_accent(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z3-MCCM5W8u"
      },
      "source": [
        "### Lexical Analysis Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACIdpcuhxblE"
      },
      "source": [
        "# Convert all characters to lowercase from list of tokenized words\n",
        "def to_lowercase(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "# Remove punctuation from list of tokenized words\n",
        "def remove_punctuation(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "# Replace all interger occurrences in list of tokenized words with textual representation\n",
        "def remove_numbers(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'\\d+','',word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def lexical_analysis(words):\n",
        "    words = remove_accent(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_numbers(words)\n",
        "    return words"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzTRyHgt5lNG"
      },
      "source": [
        "### Stop word elimination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYLlmSTE5qRU"
      },
      "source": [
        "def remove_stopwords(words):\n",
        "    new_words = []\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            new_words.append(word)\n",
        "    return new_words"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJZ_B9nD_27-"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RodgNAJf_4w2"
      },
      "source": [
        "def stem_words(words):\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stONOksgIM4x"
      },
      "source": [
        "## Step-by-step document preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfKX4iI0IbFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3adbbdde-d6ed-4047-a523-9b5821b52a78"
      },
      "source": [
        "vocabulary = []\n",
        "for filename in os.listdir(os.getcwd()+\"/Docs\"):\n",
        "    with open(os.path.join(os.getcwd()+\"/Docs\",filename),encoding=\"utf8\", errors='ignore') as rf:\n",
        "        \n",
        "        print(\"size of\",filename,\":\",os.stat(os.getcwd()+\"/Docs/\"+filename).st_size,\"bytes\")\n",
        "        \n",
        "        processed_doc_name = filename\n",
        "        \n",
        "        sample = rf.read()\n",
        "\n",
        "        sample = strip_html(sample)\n",
        "        # removal of punctuations\n",
        "        sample = sample.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
        "        words = word_tokenize(sample)\n",
        "\n",
        "        words = lexical_analysis(words)\n",
        "        with open(os.path.join(os.getcwd()+\"/processed_docs\",processed_doc_name),\"w\") as wf:\n",
        "            n = wf.write(\" \".join(words))\n",
        "        print(\"size after lexical analysis:\",os.stat(os.getcwd()+\"/processed_docs/\"+processed_doc_name).st_size,\"bytes\")\n",
        "        \n",
        "        words = remove_stopwords(words)\n",
        "        with open(os.path.join(os.getcwd()+\"/processed_docs\",processed_doc_name),\"w\") as wf:\n",
        "            n = wf.write(\" \".join(words))\n",
        "        print(\"size after removing stopwords:\",os.stat(os.getcwd()+\"/processed_docs/\"+processed_doc_name).st_size,\"bytes\")\n",
        "        \n",
        "        words = stem_words(words)\n",
        "        with open(os.path.join(os.getcwd()+\"/processed_docs\",processed_doc_name),\"w\") as wf:\n",
        "            n = wf.write(\" \".join(words))\n",
        "        print(\"size after stemming:\",os.stat(os.getcwd()+\"/processed_docs/\"+processed_doc_name).st_size,\"bytes\")\n",
        "        \n",
        "        words = remove_stopwords(words)\n",
        "        with open(os.path.join(os.getcwd()+\"/processed_docs\",processed_doc_name),\"w\") as wf:\n",
        "            n = wf.write(\" \".join(words))\n",
        "        print(\"size after removing stop words once again after stemming:\",os.stat(os.getcwd()+\"/processed_docs/\"+processed_doc_name).st_size,\"bytes\")\n",
        "        \n",
        "        vocabulary=vocabulary+words\n",
        "        print(\"\\n\\n\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of Text1.txt : 29387 bytes\n",
            "size after lexical analysis: 18891 bytes\n",
            "size after removing stopwords: 14495 bytes\n",
            "size after stemming: 11507 bytes\n",
            "size after removing stop words once again after stemming: 11437 bytes\n",
            "\n",
            "\n",
            "\n",
            "size of Text3.txt : 27201 bytes\n",
            "size after lexical analysis: 25915 bytes\n",
            "size after removing stopwords: 17099 bytes\n",
            "size after stemming: 12698 bytes\n",
            "size after removing stop words once again after stemming: 12605 bytes\n",
            "\n",
            "\n",
            "\n",
            "size of Text2.txt : 11588 bytes\n",
            "size after lexical analysis: 11006 bytes\n",
            "size after removing stopwords: 7041 bytes\n",
            "size after stemming: 5494 bytes\n",
            "size after removing stop words once again after stemming: 5423 bytes\n",
            "\n",
            "\n",
            "\n",
            "size of LargeText.txt : 674425 bytes\n",
            "size after lexical analysis: 625272 bytes\n",
            "size after removing stopwords: 420128 bytes\n",
            "size after stemming: 321472 bytes\n",
            "size after removing stop words once again after stemming: 318439 bytes\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC1KD0oJfVKG"
      },
      "source": [
        "## Add vocabulary to text file for future use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrhH-_eLfbVj"
      },
      "source": [
        "vocabulary = list(set(vocabulary))\n",
        "vocabulary.sort()\n",
        "with open(os.path.join(os.getcwd(),\"vocabulary.txt\"),\"w\") as wf:\n",
        "    wf.write(\" \".join(vocabulary))"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}