{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SreyaSalil/IR-Assignments/blob/main/IR_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BeSue9lup0P"
      },
      "source": [
        "# IR Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcMDGLsRu0jA"
      },
      "source": [
        "*Implementation of Inverted index: Construction and searching*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h26oDiKl00sP"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_JFMGz_03aM",
        "outputId": "bebabc8b-d0c8-49de-f5a5-8028c486f8df"
      },
      "source": [
        "import string\n",
        "import itertools\n",
        "import math\n",
        "import operator\n",
        "#For acessing files\n",
        "import glob \n",
        "import errno\n",
        "#To remove Numbers in text using RE\n",
        "import re\n",
        "#accent removal\n",
        "import unicodedata\n",
        "#stop word removal\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "#Stemming\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngPKXzz1OyQ"
      },
      "source": [
        "## Preprocessing Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1PwyBzj1nO1"
      },
      "source": [
        "### Read all lines in file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3QBVxmB1kks"
      },
      "source": [
        "def readFile(file):\n",
        "  line = file.read().replace(\"\\n\", \" \")\n",
        "  file.close()\n",
        "  return line"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ar-2hE1laM"
      },
      "source": [
        "### Lexical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4g5jgW91uJM"
      },
      "source": [
        "def convertToLowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "def removeNumbers(text):\n",
        "  return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def removePunctuation(text):\n",
        "  return text.translate(str.maketrans('','',string.punctuation))\n",
        "\n",
        "def removeWhitespace(text):\n",
        "  return ' '.join(text.split())\n",
        "\n",
        "def lexicalAnalysis(text):\n",
        "  text=convertToLowercase(text)\n",
        "  text=removeNumbers(text)\n",
        "  text=removePunctuation(text)\n",
        "  text=removeWhitespace(text)\n",
        "  return text"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCNilx4P1ye_"
      },
      "source": [
        "### Accent Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQKVYfkn1z3p"
      },
      "source": [
        "def removeAccent(text):\n",
        "  new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return new_text"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Edndln12UN"
      },
      "source": [
        "### Stop word elimination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7NWQs5L15dP"
      },
      "source": [
        "def eliminateStopword(tokens):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  result = [i for i in tokens if not i in stop_words and len(i) > 2]\n",
        "  return result"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNHiaoGt1_AV"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMwBPmjF2BD4"
      },
      "source": [
        "def stemming(tokens):\n",
        "  stemmer= PorterStemmer()\n",
        "  output=[stemmer.stem(word) for word in tokens]\n",
        "  return output"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXtwirLw2Lkc"
      },
      "source": [
        "### Step-by-step document preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP_zp2QJ2MLV"
      },
      "source": [
        "def preprocess_data(contents):\n",
        "  dataDict = {}\n",
        "  for content in contents:\n",
        "    textLA=lexicalAnalysis(content[1])\n",
        "    textRA=removeAccent(textLA)\n",
        "    tokens=word_tokenize(textRA)\n",
        "    tokenES=eliminateStopword(tokens)\n",
        "    stems=stemming(tokenES)\n",
        "    finalTokens=eliminateStopword(stems)\n",
        "    dataDict[content[0]] = finalTokens\n",
        "  return dataDict"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QobDkO6ZJUEh"
      },
      "source": [
        "### Query Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynndsgfw6R0i"
      },
      "source": [
        "def preprocess_queries():\n",
        "    queriesDict = {}\n",
        "    queries = open('/content/sample_data/queries.txt','r').read().split('\\n')\n",
        "    i = 1\n",
        "    for query in queries:\n",
        "        textLA=lexicalAnalysis(query)\n",
        "        textRA=removeAccent(textLA)\n",
        "        tokens=word_tokenize(textRA)\n",
        "        tokenES=eliminateStopword(tokens)\n",
        "        stems=stemming(tokenES)\n",
        "        queriesDict[i] = stems\n",
        "        i+=1\n",
        "    return queriesDict\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH8XYYYbu79f"
      },
      "source": [
        "## Generating an inverted index from documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG96MtEy1zkc"
      },
      "source": [
        "### Retrieve all the files uploaded in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVWDllyjumcI"
      },
      "source": [
        "path = '/content/sample_data/*.txt' \n",
        "files = glob.glob(path)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CbukarW15x2"
      },
      "source": [
        "### Create a list of tuples (documentNumber, documentText)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCbJcjj13UxE"
      },
      "source": [
        "uniqueTerms=set()\n",
        "fileText=[]\n",
        "i=0\n",
        "for name in files:\n",
        "  try: \n",
        "        with open(name,encoding=\"utf8\", errors='ignore') as f:\n",
        "          i=i+1\n",
        "          text=readFile(f)\n",
        "          fileText.append((name,text))\n",
        "  except IOError as exc: \n",
        "        if exc.errno != errno.EISDIR: \n",
        "            raise"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hQ8H_K62CjG"
      },
      "source": [
        "### Function to generate inverted index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTESF0IXnXsm"
      },
      "source": [
        "def generate_inverted_index(data):\n",
        "    all_words = get_vocabulary(data)\n",
        "    index = {}\n",
        "    for word in all_words:\n",
        "        for doc, tokens in data.items():\n",
        "            if word in tokens :\n",
        "                if word in index.keys():\n",
        "                    index[word].append(doc)\n",
        "                else:\n",
        "                    index[word] = [doc]\n",
        "    return index"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K40FiR5V3PVN"
      },
      "source": [
        "## Functions to calculate tf-idf values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYpxOUjgwZtZ"
      },
      "source": [
        "### Calculate tf-values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwT4PI_Qir2_"
      },
      "source": [
        "def calculate_tf(tokens):\n",
        "    tf_score = {}\n",
        "    for token in tokens:\n",
        "        tf_score[token] = tokens.count(token)\n",
        "    return tf_score"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH7c_zjPwisX"
      },
      "source": [
        "### Find Vocabulary in docs and get their frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf3Fe7Kqk7vW"
      },
      "source": [
        "def get_vocabulary(data):\n",
        "    tokens = []\n",
        "    for token_list in data.values():\n",
        "        tokens = tokens + token_list\n",
        "    fdist = nltk.FreqDist(tokens)\n",
        "    return list(fdist.keys())"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouQX0p4ewoY4"
      },
      "source": [
        "### Calculate idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSEy4bnXlCFl"
      },
      "source": [
        "def calculate_idf(data):\n",
        "    idf_score = {}\n",
        "    N = len(data)\n",
        "    all_words = get_vocabulary(data)\n",
        "    for word in all_words:\n",
        "        word_count = 0\n",
        "        for token_list in data.values():\n",
        "            if word in token_list:\n",
        "                word_count += 1\n",
        "        idf_score[word] = math.log10(N/word_count)\n",
        "    return idf_score"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFkHlb08wry3"
      },
      "source": [
        "### Calculate tf-idf values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMZ2uJJ6lD8D"
      },
      "source": [
        "def calculate_tfidf(data, idf_score):\n",
        "    scores = {}\n",
        "    for key,value in data.items():\n",
        "        scores[key] = calculate_tf(value)\n",
        "    for doc,tf_scores in scores.items():\n",
        "        for token, score in tf_scores.items():\n",
        "            tf = score\n",
        "            idf = idf_score[token]\n",
        "            tf_scores[token] = tf * idf\n",
        "    return scores"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt4Lwv8Wwwhj"
      },
      "source": [
        "### Calculate tf-idf values for queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfdTC8mRCOdA"
      },
      "source": [
        "def calculate_tfidf_queries(queries, idf_score):\n",
        "    scores = {}\n",
        "    for key, value in queries.items():\n",
        "        scores[key] = calculate_tf(value)\n",
        "    for key, tf_scores in scores.items():\n",
        "        for token, score in tf_scores.items():\n",
        "            idf = 0\n",
        "            tf = score\n",
        "            if token in idf_score.keys():\n",
        "                idf = idf_score[token]\n",
        "            tf_scores[token] = tf * idf\n",
        "    return scores"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjpRD38kJ6MT"
      },
      "source": [
        "## Searching Documents with Queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jdnHlLEKBVD"
      },
      "source": [
        "### Accept Queries from user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUNxdEEZzr_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54893f40-c4b5-4143-882a-21e1d3c2cc62"
      },
      "source": [
        "#Delete queries.txt before every run\n",
        "numQuery = input('Enter the number of queries:')\n",
        "for i in range(1,int(numQuery)+1):\n",
        "  text=input('Enter query '+str(i)+':')\n",
        "  with open('/content/sample_data/queries.txt' , 'a') as writefile:\n",
        "      writefile.write(text+'\\n')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the number of queries:2\n",
            "Enter query 1:A reptile's reponse to solar exposure\n",
            "Enter query 2:solar exposure\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLklSDcjKaf8"
      },
      "source": [
        "### Preprocess queries and documents, Generate inverted index and Calculate tf-idf values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjbTZQX3xkR-"
      },
      "source": [
        "preprocessed_data = preprocess_data(fileText)\n",
        "inverted_index = generate_inverted_index(preprocessed_data)\n",
        "idf_scores = calculate_idf(preprocessed_data)\n",
        "scores = calculate_tfidf(preprocessed_data,idf_scores)\n",
        "\n",
        "queries = preprocess_queries()\n",
        "query_scores = calculate_tfidf_queries(queries,idf_scores)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA-TCzUdKgZN"
      },
      "source": [
        "### Rank Documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3krJCZ8Cuwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9dc0440-20b1-4a4e-f811-848c9e0d2d0a"
      },
      "source": [
        "query_docs = {}\n",
        "for key, value in queries.items():\n",
        "    doc_sim = {}\n",
        "    for term in value:\n",
        "        if term in inverted_index.keys():\n",
        "            docs = inverted_index[term]\n",
        "            for doc in docs:\n",
        "                doc_score = scores[doc][term]\n",
        "                doc_length = math.sqrt(sum(x ** 2 for x in scores[doc].values()))\n",
        "                query_score = query_scores[key][term]\n",
        "                query_length = math.sqrt(sum(x ** 2 for x in query_scores[key].values()))\n",
        "                cosine_sim = (doc_score * query_score) / (doc_length * query_length)\n",
        "                if doc in doc_sim.keys():\n",
        "                    doc_sim[doc] += cosine_sim\n",
        "                else:\n",
        "                    doc_sim[doc] = cosine_sim\n",
        "    ranked = sorted(doc_sim.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    query_docs[key] = ranked\n",
        "print(query_docs)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: [('/content/sample_data/T7.txt', 0.09184233103804183), ('/content/sample_data/T8.txt', 0.03518928655476267), ('/content/sample_data/T6.txt', 0.013996072388854256), ('/content/sample_data/T1.txt', 0.010255601632123367), ('/content/sample_data/T2.txt', 0.0006022491308780927), ('/content/sample_data/T4.txt', 0.0005290543318748034), ('/content/sample_data/T3.txt', 0.00021528359743621994), ('/content/sample_data/T9.txt', 6.256533646801775e-05), ('/content/sample_data/T5.txt', 5.914322149990053e-05), ('/content/sample_data/T10.txt', 5.3409871921490944e-05)], 2: [('/content/sample_data/T6.txt', 0.01579034221575958), ('/content/sample_data/T1.txt', 0.0011921618733945019), ('/content/sample_data/T2.txt', 0.0006794563225667446), ('/content/sample_data/T4.txt', 0.0005968780897193587), ('/content/sample_data/T3.txt', 0.00024288254465337303), ('/content/sample_data/T9.txt', 7.05860934572529e-05), ('/content/sample_data/T5.txt', 6.672526986710942e-05), ('/content/sample_data/T10.txt', 6.025691579102141e-05), ('/content/sample_data/T7.txt', 4.806901413634804e-05)], 3: []}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GvE7SdzKkBQ"
      },
      "source": [
        "### Output Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh795gkHEdp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a45045f-6633-4a52-b381-464c69d2fd8e"
      },
      "source": [
        "for i in range(1, len(query_docs) + 1):\n",
        "  docs = query_docs[i][:10]\n",
        "  doc_list = [x[0] for x in docs]\n",
        "  if len(doc_list):\n",
        "    print(\"Rank of documents for query \"+str(i)+\" :\")\n",
        "    print(doc_list)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rank of documents for query 1 :\n",
            "['/content/sample_data/T7.txt', '/content/sample_data/T8.txt', '/content/sample_data/T6.txt', '/content/sample_data/T1.txt', '/content/sample_data/T2.txt', '/content/sample_data/T4.txt', '/content/sample_data/T3.txt', '/content/sample_data/T9.txt', '/content/sample_data/T5.txt', '/content/sample_data/T10.txt']\n",
            "Rank of documents for query 2 :\n",
            "['/content/sample_data/T6.txt', '/content/sample_data/T1.txt', '/content/sample_data/T2.txt', '/content/sample_data/T4.txt', '/content/sample_data/T3.txt', '/content/sample_data/T9.txt', '/content/sample_data/T5.txt', '/content/sample_data/T10.txt', '/content/sample_data/T7.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}